#pragma once
#include <device_launch_parameters.h>
#include <cuda_runtime.h>
#include <vector>

#define CudaStatusCheck(Status,Msg) {if(Status != cudaSuccess){std::cout<<Msg << " "<< cudaGetErrorString(Status) << "\nIn file: " << __FILE__ << "\nOn line number: " << __LINE__;goto Error;}}

//struct for holding MatMul memory and execution state
struct CudaMatMulHandle
{
    float* dev_a;
    float* dev_b;
    float* dev_c;

    size_t aRows;
    size_t inner;
    size_t bCols;
};


/*
Converts a 2d vector into a row - major 1d Vector.
Typically used for smaller Matices where we want an easier way to visualize whats going on prior to kernal input.
Larger matrices are Generated as flat to avoid double work. Don't try to "flatten" matrices generated by generateMatrix
*/
std::vector<float> flatten(const std::vector< std::vector<int> >& tdv);
/*
Used To Generate Large Matrices. Matrices are Generated as a flatened vector.
ROW and COL size are not preserved in matix data so make sure to store before creation to preserve Size info needed for certain Kernal Calls
*/
std::vector<float> generateMatrix(size_t rows, size_t cols, int seed = 1);
/*
Used To validate results of Matrix Addition kernel. 
Should not be included in runtime total used for kernal performance analytics,
*/
bool validateAdd(const float* inputA, const float* inputB, const float* result, size_t N);

/*
Used To Validate Matrix Multiply kernel
Should not be included in runtime total used for kernal performance analytics,
Don't use this on Larger Matrices(size tbd) it will take a long time
*/
bool validateMultiply(const float* inputA, const float* inputB, const float* inputC, size_t aRows, size_t inner, size_t bCols);

//Resource Allocators and Managers

/*
* Creates Context for MatMul Operations
* Used for Allocating the proper amount of device Memory and error checking along the way
*/
cudaError_t createMatMulContext(CudaMatMulHandle& context, size_t aRows, size_t inner, size_t bCols);
/*
* Fills Device Memory with Inputs (Host->Device)
*/
cudaError_t copyMatMalInputsToDevice(CudaMatMulHandle& context, const float* host_A, const float* host_B);
/*
* Copies Results back to Host Memory 
*/
cudaError_t copyMatMulOutputToHost(CudaMatMulHandle& context, float* host_C);

void destroyMatMulContext(CudaMatMulHandle& ctx);