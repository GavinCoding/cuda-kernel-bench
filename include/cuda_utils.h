#pragma once
#include <device_launch_parameters.h>
#include <cuda_runtime.h>
#include <cublas_v2.h>
#include <vector>



//struct for holding MatMul memory and execution state
struct CudaMatMulHandle
{
    float* dev_a;
    float* dev_b;
    float* dev_c;

    size_t aRows;
    size_t inner;
    size_t bCols;

    cublasHandle_t cublasHandle;
};
struct CudaReduceHandle
{
    float* dev_input;
    float* dev_output;

    float* dev_blockSums;
    size_t numBlocks;

    size_t N;

    //Handler for thrust call???

};


/*
Converts a 2d vector into a row - major 1d Vector.
Typically used for smaller Matices where we want an easier way to visualize whats going on prior to kernal input.
Larger matrices are Generated as flat to avoid double work. Don't try to "flatten" matrices generated by generateMatrix
*/
std::vector<float> flatten(const std::vector< std::vector<int> >& tdv);
/*
Used To Generate Large Matrices. Matrices are Generated as a flatened vector.
ROW and COL size are not preserved in matix data so make sure to store before creation to preserve Size info needed for certain Kernal Calls
*/
std::vector<float> generateMatrix(size_t rows, size_t cols, int seed = 1);
/*
Used To validate results of Matrix Addition kernel. 
Should not be included in runtime total used for kernal performance analytics,
*/
bool validateAdd(const float* inputA, const float* inputB, const float* result, size_t N);

/*
Used To Validate Matrix Multiply kernel
Should not be included in runtime total used for kernal performance analytics,
Don't use this on Larger Matrices(size tbd) it will take a long time
*/
bool validateMultiply(const float* inputA, const float* inputB, const float* inputC, size_t aRows, size_t inner, size_t bCols);

//Resource Allocators and Managers

//Creates Context for Reduction operations
cudaError_t createReduceContext(CudaReduceHandle& context, size_t N);


//Fills Device Memory with Inputs (Host->Device)
cudaError_t copyReduceInputsToDevice(CudaReduceHandle& context, const float* input);

//Copies Results back to Host Memory
cudaError_t copyReduceOutputToHost(CudaReduceHandle& context, float* output);

void destroyReduceContext(CudaReduceHandle& ctx);

//Creates Context for MatMul Operations
cudaError_t createMatMulContext(CudaMatMulHandle& context, size_t aRows, size_t inner, size_t bCols);

//Fills Device Memory with Inputs (Host->Device)
cudaError_t copyMatMalInputsToDevice(CudaMatMulHandle& context, const float* host_A, const float* host_B);

// Copies Results back to Host Memory 
cudaError_t copyMatMulOutputToHost(CudaMatMulHandle& context, float* host_C);

void destroyMatMulContext(CudaMatMulHandle& ctx);




/*
* int return used for passing event success. Used for early exits in cases where Errors should stop progress.
* Return ignored for in utility functions since caller is responsible for error handling
*/
int CheckError(cudaError_t status, const char* errorMsg, const char* file, int line);